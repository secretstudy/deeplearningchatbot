{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe04d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "414/414 [==============================] - 9s 19ms/step - loss: 1.3204 - accuracy: 0.4805 - val_loss: 0.8543 - val_accuracy: 0.5825\n",
      "Epoch 2/5\n",
      "414/414 [==============================] - 7s 18ms/step - loss: 0.8939 - accuracy: 0.6046 - val_loss: 0.7197 - val_accuracy: 0.6523\n",
      "Epoch 3/5\n",
      "414/414 [==============================] - 8s 18ms/step - loss: 0.8490 - accuracy: 0.6387 - val_loss: 0.6620 - val_accuracy: 0.6595\n",
      "Epoch 4/5\n",
      "414/414 [==============================] - 7s 18ms/step - loss: 0.7158 - accuracy: 0.7540 - val_loss: 0.5036 - val_accuracy: 0.8363\n",
      "Epoch 5/5\n",
      "414/414 [==============================] - 7s 18ms/step - loss: 0.5476 - accuracy: 0.8625 - val_loss: 0.3135 - val_accuracy: 0.9484\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.9399\n",
      "Accuracy: 93.993235\n",
      "loss : 0.359129\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding , Dense, Dropout,Conv1D,GlobalMaxPool1D, concatenate\n",
    "\n",
    "# 데이터 읽어오기 1\n",
    "train_file = './chatbot_data.csv'\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# 단어 인덱스 시퀸스 벡터 2\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.oov_token = None\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "MAX_SEQ_LEN = 15 # 단어 시퀸스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen = MAX_SEQ_LEN,padding='post')\n",
    "\n",
    "#학습용, 검증용, 테스트용 데이터셋 생성 3\n",
    "#학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.take(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "#하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1 #전체 단어수\n",
    "\n",
    "#CNN 모델 정의 4\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "\n",
    "# 3,4,5-gram 이후 합치기\n",
    "concat = concatenate([pool1,pool2,pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "predictions = Dense(128, activation=tf.nn.relu)(logits)\n",
    "\n",
    "# 모델 생성 5\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습 6 \n",
    "model.fit(train_ds, validation_data = val_ds, epochs=EPOCH, verbose = 1)\n",
    "\n",
    "# 모델 평가(테스트 데이터셋 이용) 7 \n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss : %f' % (loss))\n",
    "\n",
    "# 모델 저장 8\n",
    "model.save('cnn_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
